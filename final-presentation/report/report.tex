\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % Impostato su Inglese
\usepackage{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}

\geometry{margin=2.5cm}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeblue}{rgb}{0.1,0.1,0.8}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single
}

\lstset{style=mystyle}

\title{\textbf{Technical Report: Synapse \\LLM based system for Learning}}
\author{Synapse Group}
\date{\today}

\begin{document}

\maketitle
\begin{abstract}
\textit{Synapse} is an LLM-based system designed to support university studies through the automatic generation of educational material. This report explores the system architecture, focusing on the integration of agentic design patterns such as \textbf{Retrieval-Augmented Generation}, \textbf{Reflection}, \textbf{Few-Shot}, \textbf{Tool Use}, and advanced export functionalities.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
The evolution of Large Language Models (LLMs) has opened new possibilities in the field of assisted education. However, the "naive" use of these models presents significant limitations: hallucinations, superficiality in responses, and a lack of adherence to specific study materials.\\
We created \textit{Synapse} to solve these problems through an architecture that combines:
\begin{itemize}
    \item \textbf{RAG:} To ground LLM responses in user-provided documents.
    \item \textbf{Reflection:} To improve the quality and correctness of generated flashcards through a draft-critique-refine cycle.
    \item \textbf{Few-Shot:} To enhance LLM performance by providing examples within prompts.
    \item \textbf{Tool Use:} By searching on the web to supplement user-provided knowledge (if provided) with up-to-date information from the web.
    \item \textbf{Exporting Capabilities:} To produce flashcards compatible also with the popular spaced repetition study tool Anki.
\end{itemize}

\section{System Architecture}
The system is built in Python and follows a modular service-oriented architecture.

\subsection{Service Overview}
The logical core resides in the \texttt{services/} directory, which decouples responsibilities:
\begin{itemize}
    \item \texttt{ai\_service.py}: Abstraction for interaction with LLMs (supports Google Gemini and local models via Ollama).
    \item \texttt{rag\_service.py}: Handles document indexing, chunking, and vector retrieval.
    \item \texttt{reflection\_service.py}: Implements the logic for the draft-critique-refine cycle of the flashcard generation.
    \item \texttt{web\_search\_service.py}: Provides additional context from the web when local documents are insufficient.
    \item \texttt{export\_service.py}: Manages data conversion into interoperable formats (CSV, Anki).
\end{itemize}

\subsection{User Interface}
The graphical interface is built with \textbf{PyQt6}.\\
The UI structure (\texttt{ui/}) separates presentation logic (`main\_window.py`, `subject\_window.py`) from configuration dialogs (`settings\_dialog.py`).
\clearpage
\section{Agentic Design Patterns: Reflection }
Instead of passively accepting the LLM's output, the system establishes a self-improvement cycle.
The flashcard generation process follows three distinct phases, orchestrated by \texttt{reflection\_service.py}:

\subsection{1. Draft (Draft Generation)}
The LLM is instructed to act as an expert in metacognition. The prompt imposes "atomic" rules based on Andy Matuschak's principles, requiring structured JSON output.

\begin{lstlisting}[language=Python, caption=Prompt for draft generation]
prompt = f"""
You are an expert in learning and metacognition. Your goal is to create ONE 'atomic' flashcard based on Andy Matuschak's principles.

REQUIRED TOPIC (MANDATORY): {topic}

ATTENTION: The flashcard MUST ONLY be about '{topic}'.
- If the context does not contain information relevant to '{topic}', indicate that there is insufficient information.
- DO NOT create flashcards on topics other than '{topic}'.

Use ONLY information from the following context that is RELEVANT to '{topic}':
{context}

Follow these 5 ABSOLUTE RULES:
1.  **Focused**: The question (front) must relate to ONLY ONE concept or fact RELATED TO '{topic}'.
2.  **Precise**: The question must not be ambiguous. It must make it clear exactly what is required on '{topic}'.
3.  **Consistent**: The answer (back) must be the only correct answer and always the same.
4.  **Ask 'Why'**: If possible, prefer questions about 'why' or implications rather than dry definitions.
5.  **Cognitive Effort**: The answer should NOT be obvious from the question (avoid trivial clues or binary Yes/No questions).

Return the response in JSON format with this exact structure:
{{
'front': 'Atomic, precise question requiring effort on {topic}',
'back': 'Concise and accurate response based on context'
}}

Do not invent information that is not present in the context. If the context does not mention '{topic}', return:
{{
'front': 'Information not available',
'back': 'The context provided does not contain relevant information about {topic}'
}}
"""
\end{lstlisting}

Even if not highlighted in a separate section, this example shows how the prompt also used \textbf{Few-Shot} examples to guide the model towards a well-formatted JSON file.

\subsection{2. Critique (Critical Analysis)}
In this phase, the system assumes an "adversarial" role. A second prompt asks the LLM to evaluate the newly generated draft. This is not a simple review, but a validation against specific criteria.

\begin{lstlisting}[language=Python, caption=Prompt for critique]
prompt = f"""
You are an expert reviewer of educational materials who follows Andy Matuschak's principles.

Analyse this flashcard based on the context provided.

FLASHCARD:
Question: {front}
Answer: {back}

AVAILABLE CONTEXT:
{context}

Evaluate the flashcard EXCLUSIVELY according to these 5 RULES:
1.  **Focused**: Does it ask about a single concept? Or is it too broad (e.g., does it ask for a list)?
2.  **Precise**: Is it ambiguous? Is it clear what it wants?
3.  **Context**: Is the answer correct and based ONLY on the context?
4.  **Cognitive Effort**: Is the answer too obvious when reading the question?
5.  **Conceptual**: Is it a dry definition (negative) or does it ask for the 'why', a difference, or an implication (positive)?

Provide CONSTRUCTIVE criticism in 2-3 sentences.
- If the flashcard is already excellent and follows the rules, say so (e.g., "Excellent, follows all the principles").
- If it does not follow the rules, explain WHAT to improve (e.g. 'It is not focused, it asks two things. Break it down.' OR 'The question is too vague, make it more precise.' OR 'The answer is obvious, rephrase the question to require more effort.').
"""
\end{lstlisting}

\subsection{3. Refine (Refinement)}
If the critique highlights defects (detected via keyword matching such as "not focused", "vague", etc.), the system invokes the LLM again, passing:
\begin{enumerate}
    \item The original flashcard.
    \item The received critique.
    \item The original context.
\end{enumerate}
The model is forced to produce a new version that specifically resolves the reported issues.

\section{Agentic Design Patterns: RAG }
The RAG module (\texttt{rag\_service.py}) is the heart of Synapse's "memory". This technology overcomes the static knowledge limit of LLMs by providing dynamic access to user-uploaded documents (PDFs, slides).

\subsection{How it works: A Simple Example}
To understand the operation, let's consider a practical example. Imagine the user asks: \textit{"What is the Reflection pattern?"}.

\begin{enumerate}
\item \textbf{Retrieval:} The system creates the embedding of that question (converting it into a numerical vector) and searches the database (Qdrant) for text fragments (chunks) from the PDFs that are semantically closest to the question.

\item \textbf{Augmentation:} The found fragments are inserted into the system prompt. The prompt becomes similar to:

\begin{lstlisting}[language=Python, caption=Example of RAG augmented prompt]
Use ONLY the following information to answer:
[Text extracted from PDF: 'Reflection is a pattern...'].
Question: What is the Reflection pattern?"
\end{lstlisting}
    
\item \textbf{Generation:} The LLM generates the answer based exclusively on the provided context, ensuring that the explanation is faithful to the study material and not generic.
\end{enumerate}

\subsection{Recursive Chunking}
Retrieval quality depends drastically on how documents are divided (chunking). A naive approach, cutting text every fixed $N$ characters, risks splitting sentences in half or separating related concepts, making the context incomprehensible to the LLM.\\
Synapse adopts a \textbf{Recursive Character Text Splitting} strategy, which aims to preserve text semantics by respecting the natural structure of language. The algorithm does not cut arbitrarily but tries to divide the text using a hierarchy of separators, in order of priority:

\begin{enumerate}
    \item \textbf{Paragraphs (\texttt{\textbackslash n\textbackslash n}):} First attempts to divide text into complete logical blocks (paragraphs).
    \item \textbf{Lines (\texttt{\textbackslash n}):} If a paragraph is still too long to handle, it splits by lines.
    \item \textbf{Sentences (\texttt{. }):} If necessary, it goes down to the single sentence level.
    \item \textbf{Characters:} Only as a last resort does it cut a sentence in half.
\end{enumerate}

This approach ensures that each "chunk" sent to the model contains, as much as possible, a complete and coherent thought, significantly improving the quality of generated responses.

\subsection{Vector Store and Embeddings}
The system uses \textbf{Qdrant} in embedded mode (local disk storage) to store vectors. This avoids the need for complex external servers to configure.

For embeddings, the system is hybrid:
\begin{itemize}
    \item \textbf{Local (Ollama):} Uses models like \texttt{nomic-embed-text}, ensuring total privacy and offline operation.
    \item \textbf{Cloud (Gemini):} Uses Google APIs for high-performance embeddings (\texttt{gemini-embedding-001}), ideal for those without powerful hardware.
\end{itemize}

The search is performed via \textbf{Cosine Similarity}, with a configurable relevance threshold (default 0.25) to discard noisy results.

\section{Web Search Integration}
To ensure responses are always up-to-date and complete, Synapse integrates a web search module (\texttt{web\_search\_service.py}) that intervenes when local documents do not contain the necessary information.

\subsection{Service Operation}
The system does not limit itself to a simple Google search but uses a specialized tool for AI agents (in this case, \textbf{Tavily}) optimized for content extraction.
The goal is not to provide links to the user, but to retrieve high-quality \textit{raw context} to inject into the LLM prompt.

The process takes place in three steps:
\begin{itemize}
    \item \textbf{Query Optimization:} The LLM reformulates the user's question into an optimized search query (e.g., from "who is the current president?" to "current president of Italy 2024").
    \item \textbf{Content Extraction:} The search engine visits the most relevant pages in parallel, removing ads, menus, and boilerplate, and extracting only informative text.
    \item \textbf{Context Aggregation:} Text fragments extracted from different sources are aggregated into a single context block passed to the LLM for final response generation.
\end{itemize}

\subsection{Fallback Strategy}
The service is designed to be resilient:\\
\begin{enumerate}
\item \textbf{AI Engine (Tavily):} Used as the primary source for its ability to aggregate and clean data from multiple web sources in real-time.
\item \textbf{Wikipedia API:} If the primary service is unreachable or not configured, the system automatically falls back to public Wikipedia APIs to obtain general definitions and concepts.
\end{enumerate}

\section{Export and Interoperability}
To be truly useful, the generated material must be usable in daily study tools. The \texttt{export\_service.py} service handles this need.

\subsection{Anki Package Generation (.apkg)}
The most advanced feature is the direct creation of packages for \textbf{Anki}, the most widespread spaced repetition software.
The code interacts directly with Anki's internal SQLite database:

\begin{lstlisting}[language=Python, caption=Anki database structure creation]
cursor.execute('''
    CREATE TABLE notes (
        id INTEGER PRIMARY KEY,
        guid TEXT NOT NULL,
        mid INTEGER NOT NULL,
        ...
        flds TEXT NOT NULL,  -- Contains Front and Back separated
        ...
    )
''')
\end{lstlisting}

The system generates a valid \texttt{.apkg} file that includes:
\begin{itemize}
    \item The database structure (\texttt{collection.anki2}).
    \item Card models (custom CSS for clean visualization).
    \item Decks organized by subject.
\end{itemize}

This allows the user to import hundreds of flashcards into Anki with a double click, maintaining formatting and tags.

\subsection{Export to Standard Formats}
In addition to the Anki format, the system supports export to standard formats such as CSV (Comma-Separated Values) and TSV (Tab-Separated Values). These formats allow maximum interoperability with spreadsheets, databases, and other flashcard management software, offering users flexibility in using the generated material according to their needs.

\end{document}