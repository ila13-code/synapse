\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}

\geometry{margin=2.5cm}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeblue}{rgb}{0.1,0.1,0.8}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single
}

\lstset{style=mystyle}

\title{\textbf{Report Tecnico: Synapse - Neurosymbolic AI e RAG per l'Apprendimento}}
\author{Gruppo Synapse}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Questo documento fornisce un'analisi tecnica approfondita di \textit{Synapse}, un sistema intelligente progettato per supportare lo studio universitario attraverso la generazione automatica di materiale didattico. Il report esplora l'architettura del sistema, focalizzandosi sull'integrazione di tecnologie \textbf{Retrieval-Augmented Generation (RAG)}, metodologie \textbf{Neurosymbolic} (Reflection Pattern), integrazione con servizi di ricerca web e funzionalità di esportazione avanzate.
\end{abstract}

\tableofcontents
\newpage

\section{Introduzione}
L'evoluzione dei Large Language Models (LLM) ha aperto nuove possibilità nel campo dell'educazione assistita. Tuttavia, l'uso "naive" di questi modelli presenta limitazioni significative: allucinazioni, superficialità nelle risposte e mancanza di aderenza ai materiali di studio specifici. 

\textit{Synapse} nasce per risolvere questi problemi attraverso un'architettura ibrida che combina:
\begin{itemize}
    \item \textbf{Componente Neurale:} La capacità generativa e di comprensione del linguaggio naturale degli LLM (Gemini, Ollama).
    \item \textbf{Componente Simbolica:} Regole logiche, vincoli strutturali e flussi di controllo deterministici che guidano l'AI.
    \item \textbf{Knowledge Retrieval:} Un sistema RAG per ancorare le risposte a documenti verificati.
\end{itemize}

\section{Architettura del Sistema}
Il sistema è costruito in Python e segue un'architettura modulare a servizi.

\subsection{Panoramica dei Servizi}
Il core logico risiede nella directory \texttt{services/}, che disaccoppia le responsabilità:
\begin{itemize}
    \item \texttt{ai\_service.py}: Astrazione per l'interazione con gli LLM (supporta Google Gemini e modelli locali via Ollama).
    \item \texttt{rag\_service.py}: Gestisce l'indicizzazione dei documenti, il chunking e il retrieval vettoriale.
    \item \texttt{reflection\_service.py}: Implementa la logica neurosymbolic per la generazione e validazione delle flashcard.
    \item \texttt{web\_search\_service.py}: Fornisce contesto aggiuntivo dal web quando i documenti locali non sono sufficienti.
    \item \texttt{export\_service.py}: Gestisce la conversione dei dati in formati interoperabili (CSV, Anki).
\end{itemize}

\subsection{Interfaccia Utente}
L'interfaccia grafica è realizzata con \textbf{PyQt6}, offrendo un'esperienza desktop nativa e reattiva. La struttura UI (\texttt{ui/}) separa la logica di presentazione (`main\_window.py`, `subject\_window.py`) dai dialoghi di configurazione (`settings\_dialog.py`).

\section{Neurosymbolic AI: Il Pattern di Reflection}
Una delle innovazioni principali di Synapse è l'implementazione del pattern di \textbf{Reflection}. Invece di accettare passivamente l'output dell'LLM, il sistema instaura un ciclo di auto-miglioramento.

\subsection{Il Ciclo Draft-Critique-Refine}
Il processo di generazione di una flashcard segue tre fasi distinte, orchestrate da \texttt{reflection\_service.py}:

\subsubsection{1. Draft (Generazione della Bozza)}
L'LLM viene istruito per agire come un esperto di metacognizione. Il prompt impone regole "atomiche" basate sui principi di Andy Matuschak, richiedendo un output JSON strutturato.

\begin{lstlisting}[language=Python, caption=Prompt per la generazione della bozza]
prompt = f"""Sei un esperto di apprendimento e metacognizione...
Segui queste 5 REGOLE ASSOLUTE:
1. Focalizzata: La domanda deve riguardare UN SOLO concetto.
2. Precisa: Non deve essere ambigua.
3. Coerente: La risposta deve essere l'unica corretta.
4. Chiedi il "Perché": Preferisci domande sulle implicazioni.
5. Sforzo Cognitivo: La risposta NON deve essere intuibile dalla domanda.

Restituisci la risposta in formato JSON..."""
\end{lstlisting}

\subsubsection{2. Critique (Analisi Critica)}
In questa fase, il sistema assume un ruolo "avversario". Un secondo prompt chiede all'LLM di valutare la bozza appena generata. Non si tratta di una semplice revisione, ma di una validazione rispetto a criteri specifici.

\begin{lstlisting}[language=Python, caption=Prompt per la critica]
prompt = f"""Sei un critico esperto...
Valuta la flashcard ESCLUSIVAMENTE secondo queste 5 REGOLE:
1. Focalizzata: Chiede un solo concetto? O è troppo ampia?
2. Precisa: È ambigua?
3. Contesto: La risposta è corretta e basata SOLO sul contesto?
...
Fornisci una critica COSTRUTTIVA in 2-3 frasi."""
\end{lstlisting}

\subsubsection{3. Refine (Raffinamento)}
Se la critica evidenzia difetti (rilevati tramite keyword matching come "non focalizzata", "vaga", ecc.), il sistema invoca nuovamente l'LLM passando:
\begin{enumerate}
    \item La flashcard originale.
    \item La critica ricevuta.
    \item Il contesto originale.
\end{enumerate}
Il modello è forzato a produrre una nuova versione che risolva specificamente i problemi segnalati.

\section{Retrieval-Augmented Generation (RAG)}
Il modulo RAG (\texttt{rag\_service.py}) è il cuore della "memoria" di Synapse. Questa tecnologia permette di superare il limite della conoscenza statica degli LLM, fornendo accesso dinamico ai documenti caricati dall'utente (PDF, slide).

\subsection{Come funziona: Un Esempio Semplice}
Per comprendere il funzionamento, consideriamo un esempio pratico. Immaginiamo che l'utente chieda: \textit{"Cos'è il pattern di Reflection?"}.

\begin{enumerate}
    \item \textbf{Retrieval (Recupero):} Il sistema converte la domanda in un vettore numerico (embedding) e cerca nel database (Qdrant) i frammenti di testo (chunks) dei PDF che sono semanticamente più vicini alla domanda.
    \item \textbf{Augmentation (Arricchimento):} I frammenti trovati vengono inseriti nel prompt di sistema. Il prompt diventa simile a: 
    \begin{quote}
    \textit{"Usa SOLO le seguenti informazioni per rispondere: [Testo estratto dal PDF: 'La Reflection è un pattern...']. Domanda: Cos'è il pattern di Reflection?"}
    \end{quote}
    \item \textbf{Generation (Generazione):} L'LLM genera la risposta basandosi esclusivamente sul contesto fornito, garantendo che la spiegazione sia fedele al materiale didattico e non generica.
\end{enumerate}

\subsection{Chunking Ricorsivo}
La qualità del retrieval dipende drasticamente da come i documenti vengono suddivisi (chunking). Un approccio ingenuo, che taglia il testo ogni $N$ caratteri fissi, rischia di spezzare frasi a metà o separare concetti legati tra loro, rendendo il contesto incomprensibile per l'LLM.

Synapse adotta una strategia di \textbf{Recursive Character Text Splitting}, che mira a preservare la semantica del testo rispettando la struttura naturale del linguaggio. L'algoritmo non taglia arbitrariamente, ma cerca di dividere il testo usando una gerarchia di separatori, in ordine di priorità:

\begin{enumerate}
    \item \textbf{Paragrafi (\texttt{\textbackslash n\textbackslash n}):} Tenta prima di dividere il testo in blocchi logici completi (paragrafi).
    \item \textbf{Linee (\texttt{\textbackslash n}):} Se un paragrafo è ancora troppo lungo per essere gestito, lo suddivide per righe.
    \item \textbf{Frasi (\texttt{. }):} Se necessario, scende a livello di singola frase.
    \item \textbf{Caratteri:} Solo come ultima risorsa taglia a metà una frase.
\end{enumerate}

Questo approccio garantisce che ogni "chunk" inviato al modello contenga, per quanto possibile, un pensiero completo e coerente, migliorando significativamente la qualità delle risposte generate.

\subsection{Vector Store e Embedding}
Il sistema utilizza \textbf{Qdrant} in modalità embedded (salvataggio su disco locale) per memorizzare i vettori. Questo evita la necessità di server esterni complessi da configurare.

Per gli embedding, il sistema è ibrido:
\begin{itemize}
    \item \textbf{Locale (Ollama):} Utilizza modelli come \texttt{nomic-embed-text}, garantendo privacy totale e funzionamento offline.
    \item \textbf{Cloud (Gemini):} Utilizza le API di Google per embedding ad alte prestazioni (\texttt{gemini-embedding-001}), ideale per chi non ha hardware potente.
\end{itemize}

La ricerca avviene tramite \textbf{Cosine Similarity}, con una soglia di rilevanza configurabile (default 0.25) per scartare risultati rumorosi.

\section{Integrazione Ricerca Web}
Per garantire che le risposte siano sempre aggiornate e complete, Synapse integra un modulo di ricerca web (\texttt{web\_search\_service.py}) che interviene quando i documenti locali non contengono le informazioni necessarie.

\subsection{Funzionamento del Servizio}
Il sistema non si limita a effettuare una semplice ricerca su Google, ma utilizza uno strumento specializzato per agenti AI (in questo caso, \textbf{Tavily}) ottimizzato per l'estrazione di contenuti. 
L'obiettivo non è fornire link all'utente, ma recuperare \textit{contesto grezzo} di alta qualità da iniettare nel prompt dell'LLM.

Il processo avviene in tre step:
\begin{itemize}
    \item \textbf{Query Optimization:} L'LLM riformula la domanda dell'utente in una query di ricerca ottimizzata (es. da "chi è il presidente attuale?" a "current president of Italy 2024").
    \item \textbf{Content Extraction:} Il motore di ricerca visita le pagine più rilevanti in parallelo, rimuovendo pubblicità, menu e boilerplate, ed estraendo solo il testo informativo.
    \item \textbf{Context Aggregation:} I frammenti di testo estratti da diverse fonti vengono aggregati in un unico blocco di contesto che viene passato all'LLM per la generazione della risposta finale.
\end{itemize}

\subsection{Strategia di Fallback}
Il servizio è progettato per essere resiliente:
1.  **Motore AI (Tavily):** Utilizzato come fonte primaria per la sua capacità di aggregare e pulire i dati da molteplici fonti web in tempo reale.
2.  **Wikipedia API:** Se il servizio primario non è raggiungibile o configurato, il sistema effettua automaticamente un fallback sulle API pubbliche di Wikipedia per ottenere definizioni e concetti generali.

\section{Esportazione e Interoperabilità}
Per essere veramente utile, il materiale generato deve essere utilizzabile negli strumenti di studio quotidiani. Il servizio \texttt{export\_service.py} gestisce questa necessità.

\subsection{Generazione Pacchetti Anki (.apkg)}
La funzionalità più avanzata è la creazione diretta di pacchetti per \textbf{Anki}, il software di ripetizione spaziata più diffuso.
Il codice interagisce direttamente con il database SQLite interno di Anki:

\begin{lstlisting}[language=Python, caption=Creazione struttura database Anki]
cursor.execute('''
    CREATE TABLE notes (
        id INTEGER PRIMARY KEY,
        guid TEXT NOT NULL,
        mid INTEGER NOT NULL,
        ...
        flds TEXT NOT NULL,  -- Contiene Front e Back separati
        ...
    )
''')
\end{lstlisting}

Il sistema genera un file \texttt{.apkg} valido che include:
\begin{itemize}
    \item La struttura del database (\texttt{collection.anki2}).
    \item I modelli di carte (CSS personalizzato per una visualizzazione pulita).
    \item I deck organizzati per materia.
\end{itemize}

Questo permette all'utente di importare centinaia di flashcard in Anki con un doppio click, mantenendo formattazione e tag.



\end{document}
