\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel} % Impostato su Inglese
\usepackage{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}

\geometry{margin=2.5cm}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeblue}{rgb}{0.1,0.1,0.8}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single
}

\lstset{style=mystyle}

\title{\textbf{Technical Report: Synapse - Neurosymbolic AI and RAG for Learning}}
\author{Synapse Group}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides an in-depth technical analysis of \textit{Synapse}, an intelligent system designed to support university studies through the automatic generation of educational material. The report explores the system architecture, focusing on the integration of \textbf{Retrieval-Augmented Generation (RAG)} technologies, \textbf{Neurosymbolic} methodologies (Reflection Pattern), integration with web search services, and advanced export functionalities.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
The evolution of Large Language Models (LLMs) has opened new possibilities in the field of assisted education. However, the "naive" use of these models presents significant limitations: hallucinations, superficiality in responses, and a lack of adherence to specific study materials.

\textit{Synapse} was created to solve these problems through a hybrid architecture that combines:
\begin{itemize}
    \item \textbf{Neural Component:} The generative capacity and natural language understanding of LLMs (Gemini, Ollama).
    \item \textbf{Symbolic Component:} Logical rules, structural constraints, and deterministic control flows that guide the AI.
    \item \textbf{Knowledge Retrieval:} A RAG system to anchor responses to verified documents.
\end{itemize}

\section{System Architecture}
The system is built in Python and follows a modular service-oriented architecture.

\subsection{Service Overview}
The logical core resides in the \texttt{services/} directory, which decouples responsibilities:
\begin{itemize}
    \item \texttt{ai\_service.py}: Abstraction for interaction with LLMs (supports Google Gemini and local models via Ollama).
    \item \texttt{rag\_service.py}: Handles document indexing, chunking, and vector retrieval.
    \item \texttt{reflection\_service.py}: Implements neurosymbolic logic for flashcard generation and validation.
    \item \texttt{web\_search\_service.py}: Provides additional context from the web when local documents are insufficient.
    \item \texttt{export\_service.py}: Manages data conversion into interoperable formats (CSV, Anki).
\end{itemize}

\subsection{User Interface}
The graphical interface is built with \textbf{PyQt6}, offering a native and responsive desktop experience. The UI structure (\texttt{ui/}) separates presentation logic (`main\_window.py`, `subject\_window.py`) from configuration dialogs (`settings\_dialog.py`).

\section{Neurosymbolic AI: The Reflection Pattern}
One of Synapse's main innovations is the implementation of the \textbf{Reflection} pattern. Instead of passively accepting the LLM's output, the system establishes a self-improvement cycle.

\subsection{The Draft-Critique-Refine Cycle}
The flashcard generation process follows three distinct phases, orchestrated by \texttt{reflection\_service.py}:

\subsubsection{1. Draft (Draft Generation)}
The LLM is instructed to act as an expert in metacognition. The prompt imposes "atomic" rules based on Andy Matuschak's principles, requiring structured JSON output.

\begin{lstlisting}[language=Python, caption=Prompt for draft generation]
prompt = f"""You are an expert in learning and metacognition...
Follow these 5 ABSOLUTE RULES:
1. Focused: The question must address ONLY ONE concept.
2. Precise: It must not be ambiguous.
3. Consistent: The answer must be the only correct one.
4. Ask "Why": Prefer questions about implications.
5. Cognitive Effort: The answer must NOT be intuitive from the question.

Return the response in JSON format..."""
\end{lstlisting}

\subsubsection{2. Critique (Critical Analysis)}
In this phase, the system assumes an "adversarial" role. A second prompt asks the LLM to evaluate the newly generated draft. This is not a simple review, but a validation against specific criteria.

\begin{lstlisting}[language=Python, caption=Prompt for critique]
prompt = f"""You are an expert critic...
Evaluate the flashcard EXCLUSIVELY according to these 5 RULES:
1. Focused: Does it ask for a single concept? Or is it too broad?
2. Precise: Is it ambiguous?
3. Context: Is the answer correct and based ONLY on the context?
...
Provide CONSTRUCTIVE criticism in 2-3 sentences."""
\end{lstlisting}

\subsubsection{3. Refine (Refinement)}
If the critique highlights defects (detected via keyword matching such as "not focused", "vague", etc.), the system invokes the LLM again, passing:
\begin{enumerate}
    \item The original flashcard.
    \item The received critique.
    \item The original context.
\end{enumerate}
The model is forced to produce a new version that specifically resolves the reported issues.

\section{Retrieval-Augmented Generation (RAG)}
The RAG module (\texttt{rag\_service.py}) is the heart of Synapse's "memory". This technology overcomes the static knowledge limit of LLMs by providing dynamic access to user-uploaded documents (PDFs, slides).

\subsection{How it works: A Simple Example}
To understand the operation, let's consider a practical example. Imagine the user asks: \textit{"What is the Reflection pattern?"}.

\begin{enumerate}
    \item \textbf{Retrieval:} The system converts the question into a numerical vector (embedding) and searches the database (Qdrant) for text fragments (chunks) from the PDFs that are semantically closest to the question.
    \item \textbf{Augmentation:} The found fragments are inserted into the system prompt. The prompt becomes similar to:
    \begin{quote}
    \textit{"Use ONLY the following information to answer: [Text extracted from PDF: 'Reflection is a pattern...']. Question: What is the Reflection pattern?"}
    \end{quote}
    \item \textbf{Generation:} The LLM generates the answer based exclusively on the provided context, ensuring that the explanation is faithful to the study material and not generic.
\end{enumerate}

\subsection{Recursive Chunking}
Retrieval quality depends drastically on how documents are divided (chunking). A naive approach, cutting text every fixed $N$ characters, risks splitting sentences in half or separating related concepts, making the context incomprehensible to the LLM.

Synapse adopts a \textbf{Recursive Character Text Splitting} strategy, which aims to preserve text semantics by respecting the natural structure of language. The algorithm does not cut arbitrarily but tries to divide the text using a hierarchy of separators, in order of priority:

\begin{enumerate}
    \item \textbf{Paragraphs (\texttt{\textbackslash n\textbackslash n}):} First attempts to divide text into complete logical blocks (paragraphs).
    \item \textbf{Lines (\texttt{\textbackslash n}):} If a paragraph is still too long to handle, it splits by lines.
    \item \textbf{Sentences (\texttt{. }):} If necessary, it goes down to the single sentence level.
    \item \textbf{Characters:} Only as a last resort does it cut a sentence in half.
\end{enumerate}

This approach ensures that each "chunk" sent to the model contains, as much as possible, a complete and coherent thought, significantly improving the quality of generated responses.

\subsection{Vector Store and Embeddings}
The system uses \textbf{Qdrant} in embedded mode (local disk storage) to store vectors. This avoids the need for complex external servers to configure.

For embeddings, the system is hybrid:
\begin{itemize}
    \item \textbf{Local (Ollama):} Uses models like \texttt{nomic-embed-text}, ensuring total privacy and offline operation.
    \item \textbf{Cloud (Gemini):} Uses Google APIs for high-performance embeddings (\texttt{gemini-embedding-001}), ideal for those without powerful hardware.
\end{itemize}

The search is performed via \textbf{Cosine Similarity}, with a configurable relevance threshold (default 0.25) to discard noisy results.

\section{Web Search Integration}
To ensure responses are always up-to-date and complete, Synapse integrates a web search module (\texttt{web\_search\_service.py}) that intervenes when local documents do not contain the necessary information.

\subsection{Service Operation}
The system does not limit itself to a simple Google search but uses a specialized tool for AI agents (in this case, \textbf{Tavily}) optimized for content extraction.
The goal is not to provide links to the user, but to retrieve high-quality \textit{raw context} to inject into the LLM prompt.

The process takes place in three steps:
\begin{itemize}
    \item \textbf{Query Optimization:} The LLM reformulates the user's question into an optimized search query (e.g., from "who is the current president?" to "current president of Italy 2024").
    \item \textbf{Content Extraction:} The search engine visits the most relevant pages in parallel, removing ads, menus, and boilerplate, and extracting only informative text.
    \item \textbf{Context Aggregation:} Text fragments extracted from different sources are aggregated into a single context block passed to the LLM for final response generation.
\end{itemize}

\subsection{Fallback Strategy}
The service is designed to be resilient:
1.  **AI Engine (Tavily):** Used as the primary source for its ability to aggregate and clean data from multiple web sources in real-time.
2.  **Wikipedia API:** If the primary service is unreachable or not configured, the system automatically falls back to public Wikipedia APIs to obtain general definitions and concepts.

\section{Export and Interoperability}
To be truly useful, the generated material must be usable in daily study tools. The \texttt{export\_service.py} service handles this need.

\subsection{Anki Package Generation (.apkg)}
The most advanced feature is the direct creation of packages for \textbf{Anki}, the most widespread spaced repetition software.
The code interacts directly with Anki's internal SQLite database:

\begin{lstlisting}[language=Python, caption=Anki database structure creation]
cursor.execute('''
    CREATE TABLE notes (
        id INTEGER PRIMARY KEY,
        guid TEXT NOT NULL,
        mid INTEGER NOT NULL,
        ...
        flds TEXT NOT NULL,  -- Contains Front and Back separated
        ...
    )
''')
\end{lstlisting}

The system generates a valid \texttt{.apkg} file that includes:
\begin{itemize}
    \item The database structure (\texttt{collection.anki2}).
    \item Card models (custom CSS for clean visualization).
    \item Decks organized by subject.
\end{itemize}

This allows the user to import hundreds of flashcards into Anki with a double click, maintaining formatting and tags.

\end{document}