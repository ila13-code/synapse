\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}

\geometry{margin=2.5cm}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codeblue}{rgb}{0.1,0.1,0.8}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single
}

\lstset{style=mystyle}

\title{\textbf{Report Tecnico: Synapse - Neurosymbolic AI e RAG per l'Apprendimento}}
\author{Gruppo Synapse}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Questo documento fornisce un'analisi tecnica approfondita di \textit{Synapse}, un sistema intelligente progettato per supportare lo studio universitario attraverso la generazione automatica di materiale didattico. Il report esplora l'architettura del sistema, focalizzandosi sull'integrazione di tecnologie \textbf{Retrieval-Augmented Generation (RAG)}, metodologie \textbf{Neurosymbolic} (Reflection Pattern), integrazione con servizi di ricerca web e funzionalità di esportazione avanzate.
\end{abstract}

\tableofcontents
\newpage

\section{Introduzione}
L'evoluzione dei Large Language Models (LLM) ha aperto nuove possibilità nel campo dell'educazione assistita. Tuttavia, l'uso "naive" di questi modelli presenta limitazioni significative: allucinazioni, superficialità nelle risposte e mancanza di aderenza ai materiali di studio specifici. 

\textit{Synapse} nasce per risolvere questi problemi attraverso un'architettura ibrida che combina:
\begin{itemize}
    \item \textbf{Componente Neurale:} La capacità generativa e di comprensione del linguaggio naturale degli LLM (Gemini, Ollama).
    \item \textbf{Componente Simbolica:} Regole logiche, vincoli strutturali e flussi di controllo deterministici che guidano l'AI.
    \item \textbf{Knowledge Retrieval:} Un sistema RAG per ancorare le risposte a documenti verificati.
\end{itemize}

\section{Architettura del Sistema}
Il sistema è costruito in Python e segue un'architettura modulare a servizi.

\subsection{Panoramica dei Servizi}
Il core logico risiede nella directory \texttt{services/}, che disaccoppia le responsabilità:
\begin{itemize}
    \item \texttt{ai\_service.py}: Astrazione per l'interazione con gli LLM (supporta Google Gemini e modelli locali via Ollama).
    \item \texttt{rag\_service.py}: Gestisce l'indicizzazione dei documenti, il chunking e il retrieval vettoriale.
    \item \texttt{reflection\_service.py}: Implementa la logica neurosymbolic per la generazione e validazione delle flashcard.
    \item \texttt{web\_search\_service.py}: Fornisce contesto aggiuntivo dal web quando i documenti locali non sono sufficienti.
    \item \texttt{export\_service.py}: Gestisce la conversione dei dati in formati interoperabili (CSV, Anki).
\end{itemize}

\subsection{Interfaccia Utente}
L'interfaccia grafica è realizzata con \textbf{PyQt6}, offrendo un'esperienza desktop nativa e reattiva. La struttura UI (\texttt{ui/}) separa la logica di presentazione (`main\_window.py`, `subject\_window.py`) dai dialoghi di configurazione (`settings\_dialog.py`).

\section{Neurosymbolic AI: Il Pattern di Reflection}
Una delle innovazioni principali di Synapse è l'implementazione del pattern di \textbf{Reflection}. Invece di accettare passivamente l'output dell'LLM, il sistema instaura un ciclo di auto-miglioramento.

\subsection{Il Ciclo Draft-Critique-Refine}
Il processo di generazione di una flashcard segue tre fasi distinte, orchestrate da \texttt{reflection\_service.py}:

\subsubsection{1. Draft (Generazione della Bozza)}
L'LLM viene istruito per agire come un esperto di metacognizione. Il prompt impone regole "atomiche" basate sui principi di Andy Matuschak, richiedendo un output JSON strutturato.

\begin{lstlisting}[language=Python, caption=Prompt per la generazione della bozza]
prompt = f"""Sei un esperto di apprendimento e metacognizione...
Segui queste 5 REGOLE ASSOLUTE:
1. Focalizzata: La domanda deve riguardare UN SOLO concetto.
2. Precisa: Non deve essere ambigua.
3. Coerente: La risposta deve essere l'unica corretta.
4. Chiedi il "Perché": Preferisci domande sulle implicazioni.
5. Sforzo Cognitivo: La risposta NON deve essere intuibile dalla domanda.

Restituisci la risposta in formato JSON..."""
\end{lstlisting}

\subsubsection{2. Critique (Analisi Critica)}
In questa fase, il sistema assume un ruolo "avversario". Un secondo prompt chiede all'LLM di valutare la bozza appena generata. Non si tratta di una semplice revisione, ma di una validazione rispetto a criteri specifici.

\begin{lstlisting}[language=Python, caption=Prompt per la critica]
prompt = f"""Sei un critico esperto...
Valuta la flashcard ESCLUSIVAMENTE secondo queste 5 REGOLE:
1. Focalizzata: Chiede un solo concetto? O è troppo ampia?
2. Precisa: È ambigua?
3. Contesto: La risposta è corretta e basata SOLO sul contesto?
...
Fornisci una critica COSTRUTTIVA in 2-3 frasi."""
\end{lstlisting}

\subsubsection{3. Refine (Raffinamento)}
Se la critica evidenzia difetti (rilevati tramite keyword matching come "non focalizzata", "vaga", ecc.), il sistema invoca nuovamente l'LLM passando:
\begin{enumerate}
    \item La flashcard originale.
    \item La critica ricevuta.
    \item Il contesto originale.
\end{enumerate}
Il modello è forzato a produrre una nuova versione che risolva specificamente i problemi segnalati.

\section{Retrieval-Augmented Generation (RAG)}
Il modulo RAG (\texttt{rag\_service.py}) è responsabile della gestione della conoscenza. Assicura che l'AI non "inventi" fatti, ma si basi sui PDF caricati dall'utente.

\subsection{Chunking Ricorsivo}
La qualità del retrieval dipende da come i documenti vengono suddivisi. Un chunking ingenuo (es. ogni 500 caratteri) può spezzare frasi a metà, perdendo il significato semantico.
Synapse implementa un \textbf{Recursive Character Text Splitting}:

\begin{lstlisting}[language=Python, caption=Logica di Chunking Ricorsivo]
separators = ["\n\n", "\n", ". ", " ", ""]

def _recursive_split(text, separators):
    # Prova a dividere con il separatore più "grande" (es. paragrafi)
    # Se i pezzi sono ancora troppo grandi, scendi nella gerarchia (es. frasi)
    # ...
\end{lstlisting}

Questo algoritmo tenta di preservare l'integrità semantica:
\begin{enumerate}
    \item Prima prova a dividere per doppi a capo (paragrafi).
    \item Se un paragrafo supera la dimensione massima, prova a dividere per singoli a capo.
    \item Se necessario, scende a livello di frase (punto).
\end{enumerate}

\subsection{Vector Store e Embedding}
Il sistema utilizza \textbf{Qdrant} in modalità embedded (salvataggio su disco locale) per memorizzare i vettori. Questo evita la necessità di server esterni complessi da configurare.

Per gli embedding, il sistema è ibrido:
\begin{itemize}
    \item \textbf{Locale (Ollama):} Utilizza modelli come \texttt{nomic-embed-text}, garantendo privacy totale e funzionamento offline.
    \item \textbf{Cloud (Gemini):} Utilizza le API di Google per embedding ad alte prestazioni (\texttt{gemini-embedding-001}), ideale per chi non ha hardware potente.
\end{itemize}

La ricerca avviene tramite \textbf{Cosine Similarity}, con una soglia di rilevanza configurabile (default 0.25) per scartare risultati rumorosi.

\section{Integrazione Web Search}
In alcuni casi, i documenti forniti potrebbero non essere sufficienti o aggiornati. Synapse integra un modulo di ricerca web (\texttt{web\_search\_service.py}) che agisce come fallback o arricchimento.

\subsection{Strategia a Due Livelli}
Il servizio implementa una logica di fallback intelligente:
1.  **Tavily API:** Se configurata, viene usata come prima scelta. Tavily è un motore di ricerca ottimizzato per agenti AI, che restituisce contesti puliti e rilevanti invece di semplici link.
2.  **Wikipedia API:** Se Tavily non è disponibile, il sistema interroga le API pubbliche di Wikipedia (sia in italiano che in inglese) per ottenere riassunti introduttivi sull'argomento.

I risultati della ricerca vengono puliti (rimozione di note, formattazione) e iniettati nel prompt dell'LLM come contesto supplementare, chiaramente demarcato.

\section{Esportazione e Interoperabilità}
Per essere veramente utile, il materiale generato deve essere utilizzabile negli strumenti di studio quotidiani. Il servizio \texttt{export\_service.py} gestisce questa necessità.

\subsection{Generazione Pacchetti Anki (.apkg)}
La funzionalità più avanzata è la creazione diretta di pacchetti per \textbf{Anki}, il software di ripetizione spaziata più diffuso.
Il codice interagisce direttamente con il database SQLite interno di Anki:

\begin{lstlisting}[language=Python, caption=Creazione struttura database Anki]
cursor.execute('''
    CREATE TABLE notes (
        id INTEGER PRIMARY KEY,
        guid TEXT NOT NULL,
        mid INTEGER NOT NULL,
        ...
        flds TEXT NOT NULL,  -- Contiene Front e Back separati
        ...
    )
''')
\end{lstlisting}

Il sistema genera un file \texttt{.apkg} valido che include:
\begin{itemize}
    \item La struttura del database (\texttt{collection.anki2}).
    \item I modelli di carte (CSS personalizzato per una visualizzazione pulita).
    \item I deck organizzati per materia.
\end{itemize}

Questo permette all'utente di importare centinaia di flashcard in Anki con un doppio click, mantenendo formattazione e tag.

\section{Conclusioni e Sviluppi Futuri}
Synapse dimostra come l'integrazione di tecniche Neurosymbolic e RAG possa elevare significativamente la qualità delle applicazioni basate su LLM. 

\textbf{Punti di forza emersi:}
\begin{itemize}
    \item La \textbf{Reflection} riduce drasticamente le risposte banali, costringendo il modello a "ragionare" sulla qualità del proprio output.
    \item Il \textbf{RAG Ricorsivo} garantisce che le risposte siano ancorate ai materiali del corso, fondamentale in ambito accademico.
    \item L'architettura \textbf{Locale-First} (Ollama, Qdrant locale) rispetta la privacy e riduce i costi.
\end{itemize}

Futuri sviluppi potrebbero includere il supporto per input multimodali (immagini nei PDF) e l'integrazione di un grafo di conoscenza (Knowledge Graph) per migliorare ulteriormente il retrieval di concetti interconnessi.

\end{document}
